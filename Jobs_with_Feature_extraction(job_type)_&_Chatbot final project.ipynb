{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashivashankars/CMPE256_Assignments/blob/main/Jobs_with_Feature_extraction(job_type)_%26_Chatbot%20final%20project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vmpiNFjJl0mI",
        "outputId": "4784d5c4-4f7c-4e3d-95aa-a0c43359f2e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.6/23.6 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q openai\n",
        "!pip install -q PyPDF2\n",
        "!pip install -q faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e9f90fb",
        "outputId": "3f83ddf6-b49f-4b3f-b419-af95e3af87f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-6.4.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Downloading pypdf-6.4.0-py3-none-any.whl (329 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m329.5/329.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-6.4.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import sys\n",
        "import re\n",
        "import pandas as pd\n",
        "import requests\n",
        "import faiss\n",
        "import numpy as np\n",
        "import gradio\n",
        "\n",
        "from openai import OpenAI\n",
        "import PyPDF2\n",
        "from google.colab import userdata\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "import gradio as gr\n",
        "import json\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# Install pypdf if not already installed\n",
        "try:\n",
        "    from pypdf import PdfReader\n",
        "except ImportError:\n",
        "    !pip install pypdf\n",
        "    from pypdf import PdfReader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9025c230",
        "outputId": "e747c678-361c-4a6e-8172-d0a3beffb5ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API Key loaded securely from Colab Secrets.\n"
          ]
        }
      ],
      "source": [
        "# Define Google Drive path for caching\n",
        "DRIVE_PATH = '/content/drive/MyDrive/job_matching_cache'\n",
        "\n",
        "# --- 1. SECURE API KEY HANDLING ---\n",
        "try:\n",
        "    # Tries to get key from Colab Secrets (Left sidebar > Key icon)\n",
        "    OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "    if not OPENAI_API_KEY:\n",
        "        raise ValueError(\"OPENAI_API_KEY not found in Colab Secrets.\")\n",
        "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "    print(\"OpenAI API Key loaded securely from Colab Secrets.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"Please click the Key icon on the left, add 'OPENAI_API_KEY', and enable notebook access.\")\n",
        "    sys.exit(1)\n",
        "\n",
        "# --- 2. CONFIGURATION & HELPER FUNCTIONS ---\n",
        "# (Manual file upload removed as it is handled by the Gradio App)\n",
        "\n",
        "FIELD_CONFIG = {\n",
        "    \"graduation_date\":      {\"id\": 1, \"text\": \"Expected Graduation Date\", \"mode\": \"interactive\"},\n",
        "    \"current_degree_major\": {\"id\": 2, \"text\": \"Current Degree AND Major (e.g. MS in CS)\", \"mode\": \"interactive\"},\n",
        "    \"current_degree_gpa\":   {\"id\": 3, \"text\": \"GPA (For Current Degree Only)\", \"mode\": \"interactive\"},\n",
        "    \"us_citizenship\":       {\"id\": 4, \"text\": \"Are you a US Citizen? (If No, specify Visa Type & Sponsorship)\", \"mode\": \"interactive\"},\n",
        "    \"programming_languages\": {\"id\": 5, \"text\": \"Programming Languages\", \"mode\": \"extract\"},\n",
        "    \"experience_software\":   {\"id\": 6, \"text\": \"Work/Project Experience Summary\", \"mode\": \"extract\"},\n",
        "    \"tools_frameworks\":      {\"id\": 7, \"text\": \"Tools & Frameworks\", \"mode\": \"extract\"},\n",
        "    \"leadership\":            {\"id\": 8, \"text\": \"Leadership Experience\", \"mode\": \"extract\"},\n",
        "    \"job_preference\":        {\"id\": 9, \"text\": \"Looking for Full-time / Internship / Both?\", \"mode\": \"interactive\"},\n",
        "    \"impact_outcomes\":       {\"id\": 10,\"text\": \"Quantifiable Impact & Key Achievements\", \"mode\": \"extract\"}\n",
        "}\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with open(pdf_path, 'rb') as file:\n",
        "            reader = PyPDF2.PdfReader(file)\n",
        "            for page_num in range(len(reader.pages)):\n",
        "                text += reader.pages[page_num].extract_text()\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting text from PDF: {e}\")\n",
        "        return None\n",
        "    return text\n",
        "\n",
        "def save_embeddings_and_index(index, prepared_job_data_to_save, drive_path):\n",
        "    print(\"\\n--- Saving embeddings and FAISS index to Google Drive ---\")\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "    os.makedirs(drive_path, exist_ok=True)\n",
        "\n",
        "    faiss_index_path = os.path.join(drive_path, 'faiss_index.bin')\n",
        "    job_data_path = os.path.join(drive_path, 'prepared_job_data.json')\n",
        "\n",
        "    try:\n",
        "        faiss.write_index(index, faiss_index_path)\n",
        "        print(f\"FAISS index saved to {faiss_index_path}\")\n",
        "\n",
        "        # Prepare prepared_job_data for JSON serialization\n",
        "        # Convert numpy arrays (job_embedding) to lists\n",
        "        serializable_job_data = []\n",
        "        for job_dict in prepared_job_data_to_save:\n",
        "            temp_job_dict = job_dict.copy()\n",
        "            if 'job_embedding' in temp_job_dict and isinstance(temp_job_dict['job_embedding'], np.ndarray):\n",
        "                temp_job_dict['job_embedding'] = temp_job_dict['job_embedding'].tolist()\n",
        "            serializable_job_data.append(temp_job_dict)\n",
        "\n",
        "        with open(job_data_path, 'w') as f:\n",
        "            json.dump(serializable_job_data, f, indent=4)\n",
        "        print(f\"Prepared job data saved to {job_data_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving cache files: {e}\")\n",
        "\n",
        "def load_embeddings_and_index(drive_path):\n",
        "    print(\"\\n--- Attempting to load embeddings and FAISS index from Google Drive ---\")\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "    faiss_index_path = os.path.join(drive_path, 'faiss_index.bin')\n",
        "    job_data_path = os.path.join(drive_path, 'prepared_job_data.json')\n",
        "\n",
        "    loaded_index = None\n",
        "    loaded_prepared_job_data = None\n",
        "\n",
        "    try:\n",
        "        if os.path.exists(faiss_index_path) and os.path.exists(job_data_path):\n",
        "            loaded_index = faiss.read_index(faiss_index_path)\n",
        "            print(f\"FAISS index loaded from {faiss_index_path}\")\n",
        "\n",
        "            with open(job_data_path, 'r') as f:\n",
        "                loaded_prepared_job_data = json.load(f)\n",
        "            print(f\"Prepared job data loaded from {job_data_path}\")\n",
        "\n",
        "            # Convert job_embedding back to numpy array\n",
        "            for job_dict in loaded_prepared_job_data:\n",
        "                if 'job_embedding' in job_dict and isinstance(job_dict['job_embedding'], list):\n",
        "                    job_dict['job_embedding'] = np.array(job_dict['job_embedding'], dtype='float32')\n",
        "\n",
        "            print(\"Cache loaded successfully.\")\n",
        "            return loaded_index, loaded_prepared_job_data\n",
        "        else:\n",
        "            raise FileNotFoundError(\"Cache files not found.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Cache files (FAISS index or prepared job data) not found in Google Drive. Will generate new ones.\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading cache files: {e}\")\n",
        "        return None, None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VGZgHNoCmc6m",
        "outputId": "63270706-b476-41cd-a0c3-0c20ed4625fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------\n",
            "--- Pre-loading Job Data and Initializing FAISS Index (with Caching) ---\n",
            "\n",
            "--- Attempting to load embeddings and FAISS index from Google Drive ---\n",
            "Mounted at /content/drive\n",
            "FAISS index loaded from /content/drive/MyDrive/job_matching_cache/faiss_index.bin\n",
            "Prepared job data loaded from /content/drive/MyDrive/job_matching_cache/prepared_job_data.json\n",
            "Cache loaded successfully.\n",
            "Data and FAISS index successfully loaded from Google Drive cache.\n",
            "Pre-loading complete.\n",
            "------------------------------\n",
            "Number of job entries in prepared_job_data: 1140\n",
            "Total vectors in FAISS index: 1140\n"
          ]
        }
      ],
      "source": [
        "print(\"------------------------------\")\n",
        "print(\"--- Pre-loading Job Data and Initializing FAISS Index (with Caching) ---\")\n",
        "\n",
        "# Attempt to load from cache first\n",
        "try:\n",
        "    loaded_index, loaded_prepared_job_data = load_embeddings_and_index(DRIVE_PATH)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading from Google Drive cache: {e}. Proceeding without cache.\")\n",
        "    loaded_index, loaded_prepared_job_data = None, None\n",
        "\n",
        "if loaded_index is not None and loaded_prepared_job_data is not None:\n",
        "    index = loaded_index\n",
        "    prepared_job_data = loaded_prepared_job_data\n",
        "    # Ensure job_type is added if loading from an older cache that didn't save it\n",
        "    for j in prepared_job_data:\n",
        "        if 'job_type' not in j:\n",
        "            r = j.get('role', '')\n",
        "            if re.search(r'intern', r, re.IGNORECASE): j['job_type'] = 'Intern'\n",
        "            elif re.search(r'new grad|graduate', r, re.IGNORECASE): j['job_type'] = 'New Grad'\n",
        "            else: j['job_type'] = 'Full-time'\n",
        "    print(\"Data and FAISS index successfully loaded from Google Drive cache.\")\n",
        "else:\n",
        "    print(\"Cache not found or failed to load. Proceeding with fresh data generation.\")\n",
        "    # --- 1. Fetch Job Postings and Parse ---\n",
        "    # Define the GitHub raw URL\n",
        "    github_raw_url = \"https://raw.githubusercontent.com/SimplifyJobs/Summer2026-Internships/dev/README.md\"\n",
        "\n",
        "    # Fetch the content of the README.md file\n",
        "    try:\n",
        "        response = requests.get(github_raw_url)\n",
        "        response.raise_for_status() # Raise an exception for HTTP errors\n",
        "        readme_content = response.text\n",
        "        print(f\"Successfully fetched README content from {github_raw_url}\")\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching README content: {e}\")\n",
        "        readme_content = \"\" # Initialize with empty string on error\n",
        "\n",
        "    # Define a regular expression pattern to find level 2 headings\n",
        "    heading_pattern = re.compile(r\"^##\\s(.+)$\", re.MULTILINE)\n",
        "\n",
        "    # Find all matching headings in the readme_content\n",
        "    section_headings = heading_pattern.findall(readme_content)\n",
        "\n",
        "    # Initialize an empty dictionary to store extracted DataFrames\n",
        "    all_sections_data = {}\n",
        "\n",
        "    # Iterate through each section heading to extract its content\n",
        "    for i, current_heading in enumerate(section_headings):\n",
        "        current_heading_full = f\"## {current_heading}\"\n",
        "        start_index = readme_content.find(current_heading_full)\n",
        "\n",
        "        if start_index == -1:\n",
        "            continue\n",
        "\n",
        "        end_index = -1\n",
        "        if i + 1 < len(section_headings):\n",
        "            next_heading_full = f\"## {section_headings[i+1]}\"\n",
        "            end_index = readme_content.find(next_heading_full, start_index + len(current_heading_full))\n",
        "\n",
        "        if end_index != -1:\n",
        "            section_content = readme_content[start_index + len(current_heading_full):end_index].strip()\n",
        "        else:\n",
        "            section_content = readme_content[start_index + len(current_heading_full):].strip()\n",
        "\n",
        "        section_dfs = [] # To store multiple tables if a section has them\n",
        "\n",
        "        soup = BeautifulSoup(section_content, 'lxml')\n",
        "        tables = soup.find_all('table')\n",
        "\n",
        "        for table in tables:\n",
        "            headers = []\n",
        "            if table.find('thead'):\n",
        "                for th in table.find('thead').find_all('th'):\n",
        "                    headers.append(th.get_text(strip=True))\n",
        "\n",
        "            data_rows = []\n",
        "            if table.find('tbody'):\n",
        "                for tr in table.find('tbody').find_all('tr'):\n",
        "                    row_values = []\n",
        "                    for idx, td in enumerate(tr.find_all('td')):\n",
        "                        if headers and idx < len(headers) and headers[idx] == 'Application':\n",
        "                            link = td.find('a')\n",
        "                            if link and 'href' in link.attrs:\n",
        "                                row_values.append(link['href'])\n",
        "                            else:\n",
        "                                row_values.append('')\n",
        "                        else:\n",
        "                            row_values.append(td.get_text(strip=True))\n",
        "\n",
        "                    if len(headers) > 0:\n",
        "                        if len(row_values) > len(headers):\n",
        "                            row_values = row_values[:len(headers)]\n",
        "                        elif len(row_values) < len(headers):\n",
        "                            row_values.extend([''] * (len(headers) - len(row_values))) # Pad if too few\n",
        "                        data_rows.append(row_values)\n",
        "\n",
        "            if headers and data_rows:\n",
        "                df = pd.DataFrame(data_rows, columns=headers)\n",
        "                section_dfs.append(df)\n",
        "\n",
        "        if section_dfs:\n",
        "            all_sections_data[current_heading] = section_dfs[0]\n",
        "        else:\n",
        "            all_sections_data[current_heading] = None\n",
        "    print(f\"Extracted data for {len(all_sections_data)} sections from GitHub README.\")\n",
        "\n",
        "    # --- 2. Prepare Job Data for Embedding ---\n",
        "    prepared_job_data = []\n",
        "\n",
        "    for section_name, df in all_sections_data.items():\n",
        "        if df is not None:\n",
        "            df_normalized = df.copy()\n",
        "            df_normalized.columns = df_normalized.columns.str.lower()\n",
        "\n",
        "            # FIXED: Renamed loop variable from 'index' to 'idx' to avoid shadowing the global FAISS index variable\n",
        "            for idx, row in df_normalized.iterrows():\n",
        "                company = row['company'] if 'company' in row and pd.notna(row['company']) and str(row['company']).strip() != '' else 'N/A'\n",
        "                role = row['role'] if 'role' in row and pd.notna(row['role']) and str(row['role']).strip() != '' else 'N/A'\n",
        "                location = row['location'] if 'location' in row and pd.notna(row['location']) and str(row['location']).strip() != '' else 'N/A'\n",
        "                application_link = row['application'] if 'application' in row and pd.notna(row['application']) and str(row['application']).strip() != '' else 'N/A'\n",
        "\n",
        "                embedding_text_parts = [\n",
        "                    f\"Company: {company}\",\n",
        "                    f\"Role: {role}\",\n",
        "                    f\"Location: {location}\",\n",
        "                    f\"Section: {section_name}\"\n",
        "                ]\n",
        "                embedding_text = \". \".join(embedding_text_parts)\n",
        "\n",
        "                job_details = row.to_dict()\n",
        "                job_details['application_link'] = application_link\n",
        "\n",
        "                if 'application' in job_details:\n",
        "                    del job_details['application']\n",
        "\n",
        "                job_details['section'] = section_name\n",
        "                job_details['description'] = 'N/A'\n",
        "                job_details['embedding_text'] = embedding_text\n",
        "\n",
        "                # Assign job_type based on role, to be saved with the job_details\n",
        "                r = job_details.get('role', '')\n",
        "                if re.search(r'intern', r, re.IGNORECASE): job_details['job_type'] = 'Intern'\n",
        "                elif re.search(r'new grad|graduate', r, re.IGNORECASE): job_details['job_type'] = 'New Grad'\n",
        "                else: job_details['job_type'] = 'Full-time'\n",
        "\n",
        "                prepared_job_data.append(job_details)\n",
        "    print(f\"Prepared {len(prepared_job_data)} job entries for embedding.\")\n",
        "\n",
        "    # --- 3. Generate Embeddings for Job Postings ---\n",
        "    EMBEDDING_MODEL = \"text-embedding-ada-002\" # Define the embedding model\n",
        "    job_embeddings = []\n",
        "\n",
        "    for i, job in enumerate(prepared_job_data):\n",
        "        try:\n",
        "            job_embedding_response = client.embeddings.create(\n",
        "                model=EMBEDDING_MODEL,\n",
        "                input=job['embedding_text']\n",
        "            )\n",
        "            job['job_embedding'] = job_embedding_response.data[0].embedding\n",
        "            job_embeddings.append(job['job_embedding'])\n",
        "            if i % 100 == 0: # Print progress every 100 jobs\n",
        "                print(f\"Generated embedding for job {i+1}/{len(prepared_job_data)}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating embedding for job {i+1}: {e}\")\n",
        "            job['job_embedding'] = None\n",
        "    print(f\"Generated embeddings for {len(job_embeddings)} job postings.\")\n",
        "\n",
        "    # --- 4. Initialize FAISS Index and Add Job Embeddings ---\n",
        "    embedding_dimension = 1536 # OpenAI's 'text-embedding-ada-002' dimension is 1536\n",
        "    index = faiss.IndexFlatL2(embedding_dimension)\n",
        "\n",
        "    valid_job_embeddings = [job['job_embedding'] for job in prepared_job_data if 'job_embedding' in job and job['job_embedding'] is not None]\n",
        "    job_embeddings_np = np.array(valid_job_embeddings).astype('float32')\n",
        "\n",
        "    if len(job_embeddings_np) > 0:\n",
        "        index.add(job_embeddings_np)\n",
        "        print(f\"FAISS index initialized and {index.ntotal} job embeddings added.\")\n",
        "    else:\n",
        "        print(\"No valid job embeddings to add to the FAISS index.\")\n",
        "\n",
        "    # Save the newly generated data to cache\n",
        "    save_embeddings_and_index(index, prepared_job_data, DRIVE_PATH)\n",
        "\n",
        "print(\"Pre-loading complete.\")\n",
        "print(\"------------------------------\")\n",
        "\n",
        "# Verification steps:\n",
        "print(f\"Number of job entries in prepared_job_data: {len(prepared_job_data)}\")\n",
        "print(f\"Total vectors in FAISS index: {index.ntotal}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "110d871c"
      },
      "source": [
        "# Task\n",
        "To debug and enhance the job matching process, I will modify the `perform_job_search` function to include detailed logging of candidate data, matching results, and output string construction. Additionally, I will improve the no-match feedback message to provide more specific reasons for zero results. This will enable verification of Gradio UI output and provide insights into the matching logic."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d3b7683"
      },
      "source": [
        "## Add Debugging Logs for Matching Criteria\n",
        "\n",
        "### Subtask:\n",
        "Insert detailed logging in `perform_job_search` to show the `current_data` (especially 'job_preference' and 'graduation_date') and `current_profile_text` being used for matching. Crucially, log the `matches` and `threshold` variables immediately after calling `find_matches_with_criteria` to confirm if any jobs are being identified at that stage.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8821ee89"
      },
      "source": [
        "**Reasoning**:\n",
        "To add debugging logs as requested, I need to modify the `perform_job_search` function in the existing code cell `dd93c5b9` by inserting print statements at the specified locations. This will help in understanding the input parameters for job matching and the immediate results after the matching process.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "e52eb88a",
        "outputId": "1a0c9ebb-e219-41a3-aceb-13fded5ac4bd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3322284446.py:569: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
            "/tmp/ipython-input-3322284446.py:587: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(\n",
            "/tmp/ipython-input-3322284446.py:587: DeprecationWarning: The 'bubble_full_width' parameter will be removed in Gradio 6.0. This parameter no longer has any effect.\n",
            "  chatbot = gr.Chatbot(\n",
            "/tmp/ipython-input-3322284446.py:587: DeprecationWarning: The default value of 'allow_tags' in gr.Chatbot will be changed from False to True in Gradio 6.0. You will need to explicitly set allow_tags=False if you want to disable tags in your chatbot.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://4a7e95a7af8e5af58d.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://4a7e95a7af8e5af58d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[DEBUG] hybrid_search_jobs: valid_jobs after filtering = 1140\n",
            "[DEBUG] hybrid_search_jobs: total results (sorted) = 1140\n",
            "[DEBUG] find_matches_with_criteria: ranked jobs count = 1140\n",
            "[DEBUG] find_matches_with_criteria: top_matches count (full list) = 1140\n",
            "[DEBUG] perform_job_search: matches count = 1140, threshold = 0.7252\n",
            "[DEBUG] perform_job_search: use_llm_fit = False\n",
            "[DEBUG] perform_job_search: job_md_list length (jobs to display) = 1140\n",
            "[DEBUG] perform_job_search: Load More visible = True\n",
            "[DEBUG] hybrid_search_jobs: valid_jobs after filtering = 57\n",
            "[DEBUG] hybrid_search_jobs: total results (sorted) = 57\n",
            "[DEBUG] find_matches_with_criteria: ranked jobs count = 57\n",
            "[DEBUG] find_matches_with_criteria: top_matches count (full list) = 57\n",
            "[DEBUG] perform_job_search: matches count = 57, threshold = 0.7401\n",
            "[DEBUG] perform_job_search: use_llm_fit = True\n",
            "[DEBUG] perform_job_search: job_md_list length (jobs to display) = 57\n",
            "[DEBUG] perform_job_search: Load More visible = True\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import re\n",
        "from PyPDF2 import PdfReader\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "from openai import OpenAI\n",
        "\n",
        "# 1. SETUP & CONFIGURATION\n",
        "# ---------------------------------------------------------\n",
        "try:\n",
        "    OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")  # userdata may be provided by your environment\n",
        "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "except Exception as e:\n",
        "    print(f\"Warning: API Key not found. {e}\")\n",
        "    client = None\n",
        "\n",
        "# Defines the schema we want to extract\n",
        "TARGET_SCHEMA = {\n",
        "    \"graduation_date\": \"NULL\",\n",
        "    \"current_degree_major\": \"NULL\",\n",
        "    \"current_degree_gpa\": \"NULL\",  # New field\n",
        "    \"us_citizenship\": \"NULL\",\n",
        "    \"visa_type\": \"NULL\",  # Re-added field for visa type\n",
        "    \"programming_languages\": \"NULL\",  # New field, replaces technical_skills\n",
        "    \"experience_software\": \"NULL\",  # New field\n",
        "    \"tools_frameworks\": \"NULL\",  # New field\n",
        "    \"leadership\": \"NULL\",  # New field\n",
        "    \"job_preference\": \"NULL\"\n",
        "}\n",
        "\n",
        "# 1b. SANITIZATION HELPERS\n",
        "# ---------------------------------------------------------\n",
        "def sanitize_string(text):\n",
        "    \"\"\"\n",
        "    Sanitizes a string by converting it to UTF-8 and back, replacing invalid characters.\n",
        "    This helps prevent TypeErrors in Gradio due to non-UTF-8 compatible strings.\n",
        "    \"\"\"\n",
        "    if text is None:\n",
        "        return \"\"\n",
        "    s = str(text)\n",
        "    encoded_bytes = s.encode(\"utf-8\", errors=\"replace\")\n",
        "    sanitized_s = encoded_bytes.decode(\"utf-8\")\n",
        "    return sanitized_s\n",
        "\n",
        "\n",
        "def sanitize_structure(obj):\n",
        "    \"\"\"\n",
        "    Recursively sanitize any strings in nested structures (dicts, lists, tuples)\n",
        "    to ensure they are valid UTF-8.\n",
        "    \"\"\"\n",
        "    if isinstance(obj, str):\n",
        "        return sanitize_string(obj)\n",
        "    elif isinstance(obj, dict):\n",
        "        return {k: sanitize_structure(v) for k, v in obj.items()}\n",
        "    elif isinstance(obj, list):\n",
        "        return [sanitize_structure(v) for v in obj]\n",
        "    elif isinstance(obj, tuple):\n",
        "        return tuple(sanitize_structure(v) for v in obj)\n",
        "    else:\n",
        "        return obj\n",
        "\n",
        "\n",
        "# 2. AGENT 1: THE EXTRACTOR\n",
        "# ---------------------------------------------------------\n",
        "def agent_extractor(file_path):\n",
        "    \"\"\"\n",
        "    Reads PDF and uses LLM to extract initial JSON data.\n",
        "    \"\"\"\n",
        "    # Step A: OCR / Text Extraction\n",
        "    try:\n",
        "        reader = PdfReader(file_path)\n",
        "        text = \"\"\n",
        "        for page in reader.pages:\n",
        "            text += page.extract_text() or \"\"\n",
        "    except Exception as e:\n",
        "        return {\"error\": f\"PDF Read Error: {e}\"}\n",
        "\n",
        "    # Step B: LLM Extraction\n",
        "    prompt = f\"\"\"\n",
        "    You are a Resume Parser Agent. Extract the following fields from the resume text.\n",
        "    Return ONLY valid JSON matching this structure exactly.\n",
        "    Use the string \"NULL\" if the information is not explicitly found.\n",
        "    For 'leadership', if found, output a list of dictionaries with 'role', 'organization', and 'description'. Otherwise, output 'NULL'.\n",
        "\n",
        "    Target Structure:\n",
        "    {json.dumps(TARGET_SCHEMA, indent=2)}\n",
        "\n",
        "    RESUME TEXT:\n",
        "    {text[:4000]}\n",
        "    \"\"\"\n",
        "\n",
        "    if client is None:\n",
        "        print(\"Extraction Error: OpenAI client is not initialized.\")\n",
        "        return TARGET_SCHEMA\n",
        "\n",
        "    try:\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\"role\": \"system\", \"content\": \"You are a JSON extractor.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt},\n",
        "            ],\n",
        "            response_format={\"type\": \"json_object\"},\n",
        "        )\n",
        "        extracted_data = json.loads(response.choices[0].message.content)\n",
        "        extracted_data = sanitize_structure(extracted_data)\n",
        "        return extracted_data\n",
        "    except Exception as e:\n",
        "        print(f\"Extraction Error: {e}\")\n",
        "        return TARGET_SCHEMA  # Return empty schema on fail to prevent crash\n",
        "\n",
        "\n",
        "# 3. AGENT 2: THE INTERVIEWER\n",
        "# ---------------------------------------------------------\n",
        "def agent_interviewer(current_data, user_response=None, current_field=None):\n",
        "    \"\"\"\n",
        "    Analyzes data, updates with user response, and determines the next question.\n",
        "    Returns: (updated_data, next_field_to_ask, question_text)\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Update data if user provided an answer\n",
        "    if user_response and current_field:\n",
        "        current_data[current_field] = user_response\n",
        "\n",
        "    # Special handling for us_citizenship = \"no\"\n",
        "    if current_field == \"us_citizenship\" and user_response and user_response.lower() in [\n",
        "        \"no\",\n",
        "        \"n\",\n",
        "        \"false\",\n",
        "    ]:\n",
        "        if current_data.get(\"visa_type\") in [\"NULL\", \"\", None, \"null\"]:\n",
        "            current_data[\"visa_type\"] = \"NULL\"  # Explicitly mark as missing to ensure it's asked next\n",
        "    # If user says yes to citizenship, ensure visa_type is not asked\n",
        "    elif current_field == \"us_citizenship\" and user_response and user_response.lower() in [\n",
        "        \"yes\",\n",
        "        \"y\",\n",
        "        \"true\",\n",
        "    ]:\n",
        "        current_data[\"visa_type\"] = \"N/A\"  # Not applicable if US Citizen\n",
        "\n",
        "    # 2. Find the next missing field\n",
        "    next_field = None\n",
        "    question = None\n",
        "\n",
        "    fields_to_check = list(TARGET_SCHEMA.keys())\n",
        "\n",
        "    for field in fields_to_check:\n",
        "        val = current_data.get(field)\n",
        "        if val in [\"NULL\", \"\", None, \"null\"]:\n",
        "            next_field = field\n",
        "\n",
        "            human_field = field.replace(\"_\", \" \").title()\n",
        "            question = (\n",
        "                f\"I noticed your resume is missing **{human_field}**. could you please provide that?\"\n",
        "            )\n",
        "\n",
        "            if field == \"us_citizenship\":\n",
        "                question = \"Are you a **US Citizen**? (Yes/No)\"\n",
        "            elif field == \"graduation_date\":\n",
        "                question = \"When is your expected **Graduation Date**?\"\n",
        "            elif field == \"current_degree_gpa\":\n",
        "                question = \"What is your **GPA** for your Current Degree Only?\"\n",
        "            elif field == \"job_preference\":\n",
        "                question = \"What is your specific **Job Preference** (e.g., Full-time / Internship / Both)?\"\n",
        "            elif field == \"visa_type\":\n",
        "                question = \"What is your **Visa Type** (e.g., H1B, F1-OPT)? Or if you don't need sponsorship, please state that.\"\n",
        "\n",
        "            break\n",
        "\n",
        "    current_data = sanitize_structure(current_data)\n",
        "    if question is not None:\n",
        "        question = sanitize_string(question)\n",
        "\n",
        "    return current_data, next_field, question\n",
        "\n",
        "\n",
        "# 4. AGENT 3: THE FINALIZER\n",
        "# ---------------------------------------------------------\n",
        "def agent_finalizer(final_data):\n",
        "    \"\"\"\n",
        "    Clean up data and generate a file for download.\n",
        "    \"\"\"\n",
        "    final_data = sanitize_structure(final_data)\n",
        "\n",
        "    filename = \"candidate_profile.json\"\n",
        "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(final_data, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "    msg = (\n",
        "        \"**Interview Complete!**\\n\\nI have generated your profile.\\n\\n```json\\n\"\n",
        "        + json.dumps(final_data, indent=2, ensure_ascii=False)\n",
        "        + \"\\n```\"\n",
        "    )\n",
        "    msg = sanitize_string(msg)\n",
        "    return filename, msg\n",
        "\n",
        "\n",
        "# Helper function to generate profile text for embedding\n",
        "def _generate_profile_text(data):\n",
        "    data = sanitize_structure(data)\n",
        "    profile_parts = []\n",
        "    for key in TARGET_SCHEMA.keys():\n",
        "        val = data.get(key)\n",
        "        if val not in [\"NULL\", \"\", None, \"null\", \"N/A\"]:\n",
        "            if isinstance(val, list) and key == \"leadership\":\n",
        "                roles = [item.get(\"role\", \"\") for item in val if item.get(\"role\")]\n",
        "                if roles:\n",
        "                    profile_parts.append(\n",
        "                        f\"{key.replace('_', ' ').title()}: {'; '.join(roles)}\"\n",
        "                    )\n",
        "            else:\n",
        "                profile_parts.append(\n",
        "                    f\"{key.replace('_', ' ').title()}: {sanitize_string(val)}\"\n",
        "                )\n",
        "    return \". \".join(profile_parts)\n",
        "\n",
        "\n",
        "# Job Matching Helper Functions\n",
        "# ---------------------------------------------------------\n",
        "def hybrid_search_jobs(resume_embedding, candidate_preferences, all_jobs):\n",
        "    pref = str(candidate_preferences.get(\"job_preference\", \"\")).lower()\n",
        "    grad_date_str = str(candidate_preferences.get(\"graduation_date\", \"\")).lower()\n",
        "\n",
        "    allowed_types = set()\n",
        "    if \"internship\" == pref:\n",
        "        allowed_types.add(\"Intern\")\n",
        "    elif \"full-time\" == pref:\n",
        "        allowed_types.update([\"Full-time\", \"New Grad\"])\n",
        "    elif \"both\" == pref:\n",
        "        allowed_types.update([\"Intern\", \"Full-time\", \"New Grad\"])\n",
        "    else:\n",
        "        import datetime\n",
        "        curr_year = datetime.datetime.now().year\n",
        "        years = [int(y) for y in re.findall(r\"\\b\\d{4}\\b\", grad_date_str)]\n",
        "        if years and max(years) > curr_year:\n",
        "            allowed_types = {\"Intern\", \"New Grad\"}\n",
        "        else:\n",
        "            allowed_types = {\"Full-time\", \"New Grad\"}\n",
        "\n",
        "    valid_jobs = [\n",
        "        j\n",
        "        for j in all_jobs\n",
        "        if j.get(\"job_embedding\") is not None\n",
        "        and j.get(\"job_type\", \"Full-time\") in allowed_types\n",
        "    ]\n",
        "    print(f\"[DEBUG] hybrid_search_jobs: valid_jobs after filtering = {len(valid_jobs)}\")\n",
        "    if not valid_jobs:\n",
        "        return []\n",
        "\n",
        "    resume_vec = np.array(resume_embedding, dtype=\"float32\").flatten()\n",
        "    job_matrix = np.array([j[\"job_embedding\"] for j in valid_jobs], dtype=\"float32\")\n",
        "\n",
        "    sims = np.dot(job_matrix, resume_vec) / (\n",
        "        np.linalg.norm(resume_vec) * np.linalg.norm(job_matrix, axis=1) + 1e-10\n",
        "    )\n",
        "\n",
        "    results = [\n",
        "        {\"job_details\": j, \"cosine_similarity\": float(s)}\n",
        "        for j, s in zip(valid_jobs, sims)\n",
        "    ]\n",
        "    results.sort(key=lambda x: x[\"cosine_similarity\"], reverse=True)\n",
        "    print(f\"[DEBUG] hybrid_search_jobs: total results (sorted) = {len(results)}\")\n",
        "    return results\n",
        "\n",
        "\n",
        "def find_matches_with_criteria(verified_prefs, profile_text, client_openai, prepared_jobs):\n",
        "    try:\n",
        "        emb = (\n",
        "            client_openai.embeddings.create(\n",
        "                model=\"text-embedding-ada-002\", input=profile_text\n",
        "            )\n",
        "            .data[0]\n",
        "            .embedding\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"Embedding generation failed: {e}\")\n",
        "        return [], 0.0\n",
        "\n",
        "    ranked = hybrid_search_jobs(emb, verified_prefs, prepared_jobs)\n",
        "    print(f\"[DEBUG] find_matches_with_criteria: ranked jobs count = {len(ranked)}\")\n",
        "\n",
        "    if not ranked:\n",
        "        return [], 0.0\n",
        "\n",
        "    # Option 3: keep full ranked list; pagination will handle display\n",
        "    top_matches = ranked\n",
        "    threshold = top_matches[-1][\"cosine_similarity\"]\n",
        "\n",
        "    print(f\"[DEBUG] find_matches_with_criteria: top_matches count (full list) = {len(top_matches)}\")\n",
        "    return top_matches, threshold\n",
        "\n",
        "\n",
        "def augment_job_with_llm(job_match, profile_text, client_openai):\n",
        "    job = job_match[\"job_details\"]\n",
        "    if client_openai is None:\n",
        "        job_match[\"llm_fit_summary\"] = \"Analysis not available (OpenAI client missing).\"\n",
        "        return job_match\n",
        "\n",
        "    try:\n",
        "        res = client_openai.chat.completions.create(\n",
        "            model=\"gpt-4o\",\n",
        "            messages=[\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": (\n",
        "                        \"Explain why candidate fits job in 2 sentences.\\n\"\n",
        "                        f\"Candidate: {profile_text}\\nJob: {job.get('embedding_text','')}\"\n",
        "                    ),\n",
        "                }\n",
        "            ],\n",
        "            max_tokens=100,\n",
        "        )\n",
        "        job_match[\"llm_fit_summary\"] = sanitize_string(\n",
        "            res.choices[0].message.content.strip()\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"LLM augmentation failed: {e}\")\n",
        "        job_match[\"llm_fit_summary\"] = \"Analysis failed.\"\n",
        "    return job_match\n",
        "\n",
        "\n",
        "# 5. GRADIO ORCHESTRATOR (UI LOGIC)\n",
        "# ---------------------------------------------------------\n",
        "def process_upload(file, history, state):\n",
        "    \"\"\"Triggered when file is uploaded\"\"\"\n",
        "    if not file:\n",
        "        return history, state, None, \"\"\n",
        "\n",
        "    extracted_data = agent_extractor(file)\n",
        "\n",
        "    initial_profile_text = _generate_profile_text(extracted_data)\n",
        "    initial_profile_text = sanitize_string(initial_profile_text)\n",
        "\n",
        "    updated_data, next_field, question = agent_interviewer(extracted_data)\n",
        "\n",
        "    # Reset job-related state on new upload\n",
        "    state[\"job_md_list\"] = []\n",
        "    state[\"job_page\"] = 0\n",
        "\n",
        "    state[\"data\"] = sanitize_structure(updated_data)\n",
        "    state[\"current_field\"] = next_field\n",
        "    state[\"profile_text\"] = initial_profile_text\n",
        "\n",
        "    history.append((None, sanitize_string(\"Resume parsed! Checking for missing info...\")))\n",
        "    if question:\n",
        "        history.append((None, sanitize_string(question)))\n",
        "        return history, state, None, \"\"\n",
        "    else:\n",
        "        fname, msg = agent_finalizer(updated_data)\n",
        "        history.append((None, sanitize_string(msg)))\n",
        "        return history, state, fname, \"\"\n",
        "\n",
        "\n",
        "def process_chat(user_msg, history, state):\n",
        "    \"\"\"Triggered when user types a message\"\"\"\n",
        "    if not user_msg:\n",
        "        return history, state, None\n",
        "\n",
        "    user_msg = sanitize_string(user_msg)\n",
        "    history.append((user_msg, None))\n",
        "\n",
        "    data = state.get(\"data\", TARGET_SCHEMA.copy())\n",
        "    current_field = state.get(\"current_field\")\n",
        "\n",
        "    updated_data, next_field, question = agent_interviewer(\n",
        "        data, user_msg, current_field\n",
        "    )\n",
        "\n",
        "    updated_profile_text = _generate_profile_text(updated_data)\n",
        "    updated_profile_text = sanitize_string(updated_profile_text)\n",
        "\n",
        "    state[\"data\"] = sanitize_structure(updated_data)\n",
        "    state[\"current_field\"] = next_field\n",
        "    state[\"profile_text\"] = updated_profile_text\n",
        "\n",
        "    # Reset job pagination on profile change\n",
        "    state[\"job_md_list\"] = []\n",
        "    state[\"job_page\"] = 0\n",
        "\n",
        "    if next_field:\n",
        "        history.append((None, sanitize_string(question)))\n",
        "        return history, state, None\n",
        "    else:\n",
        "        filename, final_msg = agent_finalizer(updated_data)\n",
        "        history.append((None, sanitize_string(final_msg)))\n",
        "        return history, state, filename\n",
        "\n",
        "\n",
        "PAGE_SIZE = 5  # number of jobs per page\n",
        "\n",
        "\n",
        "def perform_job_search(state_obj, use_llm_fit, progress=gr.Progress()):\n",
        "    \"\"\"\n",
        "    Performs the job search based on the current state data.\n",
        "    If use_llm_fit is True, only the FIRST PAGE (first 5 jobs) will call LLM for fit explanation.\n",
        "    \"\"\"\n",
        "    global client, prepared_job_data, index, DRIVE_PATH\n",
        "\n",
        "    current_data = state_obj.get(\"data\", TARGET_SCHEMA.copy())\n",
        "    current_profile_text = state_obj.get(\"profile_text\", \"\")\n",
        "\n",
        "    state_obj[\"job_md_list\"] = []\n",
        "    state_obj[\"job_page\"] = 0\n",
        "\n",
        "    hide_load_more = gr.update(visible=False)\n",
        "\n",
        "    if (\n",
        "        not current_profile_text\n",
        "        or not current_data.get(\"graduation_date\")\n",
        "        or client is None\n",
        "    ):\n",
        "        msg = (\n",
        "            \"Please upload a resume, complete the interview, \"\n",
        "            \"and ensure OpenAI client is initialized.\"\n",
        "        )\n",
        "        print(\"[DEBUG] perform_job_search: missing profile_text or graduation_date or client.\")\n",
        "        return msg, state_obj, hide_load_more\n",
        "\n",
        "    # Load job data + index if needed\n",
        "    if (\n",
        "        \"prepared_job_data\" not in globals()\n",
        "        or prepared_job_data is None\n",
        "        or \"index\" not in globals()\n",
        "        or index is None\n",
        "    ):\n",
        "        print(\"[DEBUG] perform_job_search: job data or index not found, loading from cache...\")\n",
        "        try:\n",
        "            # Assumes load_embeddings_and_index and DRIVE_PATH are defined externally\n",
        "            new_index, new_prepared_job_data = load_embeddings_and_index(DRIVE_PATH)\n",
        "            if new_index is not None and new_prepared_job_data is not None:\n",
        "                index = new_index\n",
        "                prepared_job_data = new_prepared_job_data\n",
        "                for j in prepared_job_data:\n",
        "                    if \"job_type\" not in j:\n",
        "                        r = j.get(\"role\", \"\")\n",
        "                        if re.search(r\"intern\", r, re.IGNORECASE):\n",
        "                            j[\"job_type\"] = \"Intern\"\n",
        "                        elif re.search(r\"new grad|graduate\", r, re.IGNORECASE):\n",
        "                            j[\"job_type\"] = \"New Grad\"\n",
        "                        else:\n",
        "                            j[\"job_type\"] = \"Full-time\"\n",
        "                print(\"[DEBUG] perform_job_search: job data and index successfully loaded from cache.\")\n",
        "            else:\n",
        "                print(\"[DEBUG] perform_job_search: failed to load job data or index from cache.\")\n",
        "                return (\n",
        "                    \"Error: Job data and FAISS index could not be loaded from cache. \"\n",
        "                    \"Please re-run pre-loading steps.\",\n",
        "                    state_obj,\n",
        "                    hide_load_more,\n",
        "                )\n",
        "        except Exception as e:\n",
        "            print(f\"[DEBUG] perform_job_search: exception while loading job data: {e}\")\n",
        "            return (\n",
        "                f\"Error loading job data and index from cache: {e}. \"\n",
        "                \"Please re-run pre-loading steps.\",\n",
        "                state_obj,\n",
        "                hide_load_more,\n",
        "            )\n",
        "\n",
        "    if client is None or prepared_job_data is None or index is None:\n",
        "        print(\"[DEBUG] perform_job_search: client or prepared_job_data or index still None after load.\")\n",
        "        return (\n",
        "            \"Error: OpenAI client or job data not initialized even after attempted load. \"\n",
        "            \"Please ensure previous cells run successfully.\",\n",
        "            state_obj,\n",
        "            hide_load_more,\n",
        "        )\n",
        "\n",
        "    matches, threshold = find_matches_with_criteria(\n",
        "        current_data, current_profile_text, client, prepared_job_data\n",
        "    )\n",
        "\n",
        "    print(f\"[DEBUG] perform_job_search: matches count = {len(matches)}, threshold = {threshold:.4f}\")\n",
        "    print(f\"[DEBUG] perform_job_search: use_llm_fit = {use_llm_fit}\")\n",
        "\n",
        "    if not matches:\n",
        "        feedback_message = (\n",
        "            \"No job matches found based on your profile. This might be due to:\\n\"\n",
        "            \"- No suitable job types (Intern, Full-time, New Grad) matching your preference.\\n\"\n",
        "            \"- Lack of strong skill alignment in your profile.\\n\"\n",
        "            \"- Your graduation date not aligning with available opportunities.\"\n",
        "        )\n",
        "        return feedback_message, state_obj, hide_load_more\n",
        "\n",
        "    job_md_list = []\n",
        "    total_matches = len(matches)\n",
        "\n",
        "    for i, m in enumerate(matches):\n",
        "        progress((i + 1) / total_matches, desc=f\"Preparing job {i+1}/{total_matches}...\")\n",
        "\n",
        "        # OPTION 3: Only call LLM for first PAGE_SIZE jobs if requested\n",
        "        fit_summary = \"High similarity based on your profile.\"\n",
        "        if use_llm_fit and i < PAGE_SIZE:\n",
        "            m = augment_job_with_llm(m, current_profile_text, client)\n",
        "            fit_summary = m.get(\"llm_fit_summary\", fit_summary)\n",
        "\n",
        "        job = m[\"job_details\"]\n",
        "\n",
        "        chunk = \"\"\n",
        "        chunk += (\n",
        "            f\"### {i+1}. {sanitize_string(job.get('company','N/A'))} - \"\n",
        "            f\"{sanitize_string(job.get('role','N/A'))}\\n\"\n",
        "        )\n",
        "        chunk += (\n",
        "            f\"**Loc:** {sanitize_string(job.get('location','N/A'))} | \"\n",
        "            f\"**Sim:** {m['cosine_similarity']*100:.1f}%\\n\"\n",
        "        )\n",
        "        chunk += f\"**Fit:** {sanitize_string(fit_summary)}\\n\"\n",
        "        chunk += f\"[Apply]({sanitize_string(job.get('application_link','#'))})\\n\\n---\\n\\n\"\n",
        "\n",
        "        job_md_list.append(chunk)\n",
        "\n",
        "    print(f\"[DEBUG] perform_job_search: job_md_list length (jobs to display) = {len(job_md_list)}\")\n",
        "\n",
        "    state_obj[\"job_md_list\"] = job_md_list\n",
        "    state_obj[\"job_page\"] = 1  # first page\n",
        "\n",
        "    total = len(job_md_list)\n",
        "    start = 0\n",
        "    end = min(PAGE_SIZE, total)\n",
        "    first_jobs_md = \"\".join(job_md_list[start:end])\n",
        "    header = f\"### 🎯 Found {total} matching jobs (showing {end} of {total}):\\n---\\n\\n\"\n",
        "    output_str = header + first_jobs_md\n",
        "\n",
        "    more_available = end < total\n",
        "    load_more_update = gr.update(visible=more_available)\n",
        "    print(f\"[DEBUG] perform_job_search: Load More visible = {more_available}\")\n",
        "\n",
        "    return output_str, state_obj, load_more_update\n",
        "\n",
        "\n",
        "def load_more_jobs(current_output, state_obj):\n",
        "    \"\"\"\n",
        "    Appends the next page of jobs to the existing Markdown output.\n",
        "    Returns: (updated_output, updated_state, load_more_button_update)\n",
        "    \"\"\"\n",
        "    job_md_list = state_obj.get(\"job_md_list\", [])\n",
        "    page = state_obj.get(\"job_page\", 0)\n",
        "    total = len(job_md_list)\n",
        "\n",
        "    print(f\"[DEBUG] load_more_jobs: current page = {page}, total jobs = {total}\")\n",
        "\n",
        "    if not job_md_list or page <= 0:\n",
        "        print(\"[DEBUG] load_more_jobs: no jobs to paginate or invalid page.\")\n",
        "        return current_output, state_obj, gr.update(visible=False)\n",
        "\n",
        "    start = page * PAGE_SIZE\n",
        "    end = min(start + PAGE_SIZE, total)\n",
        "\n",
        "    if start >= total:\n",
        "        print(\"[DEBUG] load_more_jobs: start index >= total, no more jobs.\")\n",
        "        return current_output, state_obj, gr.update(visible=False)\n",
        "\n",
        "    new_chunk = \"\".join(job_md_list[start:end])\n",
        "    updated_output = current_output + new_chunk\n",
        "\n",
        "    state_obj[\"job_page\"] = page + 1\n",
        "\n",
        "    more_available = end < total\n",
        "    load_more_update = gr.update(visible=more_available)\n",
        "\n",
        "    print(f\"[DEBUG] load_more_jobs: showing jobs {start+1}-{end} of {total}, Load More visible = {more_available}\")\n",
        "\n",
        "    return updated_output, state_obj, load_more_update\n",
        "\n",
        "\n",
        "# 6. UI CONSTRUCTION\n",
        "# ---------------------------------------------------------\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    state = gr.State({\n",
        "        \"data\": {},\n",
        "        \"current_field\": None,\n",
        "        \"profile_text\": \"\",\n",
        "        \"job_md_list\": [],\n",
        "        \"job_page\": 0\n",
        "    })\n",
        "\n",
        "    gr.Markdown(\"# 🤖 Agentic Resume Screener\")\n",
        "    gr.Markdown(\"Upload a resume. The agents will extract data and interview you for missing details.\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            file_upload = gr.File(label=\"1. Upload Resume (PDF)\", type=\"filepath\")\n",
        "            download_btn = gr.File(label=\"3. Download Profile\", interactive=False)\n",
        "\n",
        "        with gr.Column(scale=2):\n",
        "            chatbot = gr.Chatbot(\n",
        "                height=300, label=\"2. Interview Agent\", bubble_full_width=False\n",
        "            )\n",
        "            msg_input = gr.Textbox(\n",
        "                label=\"Your Answer\", placeholder=\"Type here and press enter...\"\n",
        "            )\n",
        "\n",
        "    with gr.Row():\n",
        "        use_llm_fit_checkbox = gr.Checkbox(\n",
        "            label=\"Explain fit with AI (slower for first page)\", value=True\n",
        "        )\n",
        "        find_jobs_btn = gr.Button(\"4. Find Jobs\", variant=\"primary\")\n",
        "        load_more_btn = gr.Button(\"Load More Jobs\", visible=False)\n",
        "\n",
        "    job_output_display = gr.Markdown(\"\", label=\"Matching Jobs\")\n",
        "\n",
        "    # Event: File Upload\n",
        "    file_upload.change(\n",
        "        fn=process_upload,\n",
        "        inputs=[file_upload, chatbot, state],\n",
        "        outputs=[chatbot, state, download_btn, job_output_display],\n",
        "    )\n",
        "\n",
        "    # Event: User Chat\n",
        "    msg_input.submit(\n",
        "        fn=process_chat,\n",
        "        inputs=[msg_input, chatbot, state],\n",
        "        outputs=[chatbot, state, download_btn],\n",
        "    ).then(lambda: \"\", outputs=msg_input)\n",
        "\n",
        "    # Event: Find Jobs (initial search + first page)\n",
        "    find_jobs_btn.click(\n",
        "        fn=perform_job_search,\n",
        "        inputs=[state, use_llm_fit_checkbox],\n",
        "        outputs=[job_output_display, state, load_more_btn],\n",
        "    )\n",
        "\n",
        "    # Event: Load More Jobs (pagination)\n",
        "    load_more_btn.click(\n",
        "        fn=load_more_jobs,\n",
        "        inputs=[job_output_display, state],\n",
        "        outputs=[job_output_display, state, load_more_btn],\n",
        "    )\n",
        "\n",
        "demo.launch(debug=True)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDYpC+l1xARKeWeAMxbti6",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}